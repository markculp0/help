
# ============
# pyspark Help
# ============

# Read files ==================================

# Read text file
poem = spark.read.text("poem.txt")

# Read CSV file
dog = spark.read.csv("dog.csv", header=True, schema=schema)

# Create DataFrame w named columns
data = spark.createDataFrame([("Jim", 20, "A"), ("Alice", 30, "B")],["name", "age", "city"])
data.show()

heights = spark.read.csv("/data/heights.csv", header=True,
  mode="DROPMALFORMED", schema=schema)

# Write files =================================

# Write to directory with one CSV file
data.coalesce(1).write.option('header','true').csv('data.csv')

# Import modules ==============================

# Import sql functions
from pyspark.sql.functions import *

from pyspark.sql.types import DoubleType

# Schema ======================================

# Define a schema
schema = StructType([StructField('Name', StringType(), True),
                     StructField('DateTime', TimestampType(), True),
                     StructField('Age', IntegerType(), True),
		     StructField('Score', DoubleType(),True)])

test = spark.read.csv("test.csv", header=False, schema=schema)
test.schema

# Convert type ================================

# Convert col to double
dog3 = dog2.withColumn("age", dog2["age"].cast("double"))

# Examine data ================================

# Count rows
dog.count()

# Get 1st row
dog.first()

# Show DataFrame
data.show()

# Filter a row ================================

# Filter the DataFrame
linesWithOne = dog.filter(dog.value.contains("1"))

# Filter out lines containing "black" 
linesWithBlack = heights.filter(heights.value.contains("black"))

# Filter DataFrame for Row w 'if' in word column
wcIf = wc2.filter(wc2.word.isin('if'))
wcIf.show()

# Drop columns in a DataFrame =================

# Drop column fm DataFrame:
# Creates a list of Rows
wcRow = wc2.drop('count').collect()

# Then create DataFrame
wc3 = spark.createDataFrame(wcRow, schema=StructType([StructField('word', StringType(), True)]))

# Drop two columns fm DataFrame
dataRow = data.drop('age','city').collect()

data3 = spark.createDataFrame(dataRow, schema=StructType([StructField('name', StringType(), True)]))

# Sum a row ===================================

# Sum a row
data.groupBy().sum().collect()
#> [Row(sum(age)=50)]

data.groupBy().sum().collect()[0][0]
#> 50

# Return DataFrame with total column
from pyspark.sql import functions as F
res = data.unionAll(
    data.select([
        F.lit('All').alias('name'), # create a cloumn named 'name' and filled with 'All'
        F.sum(data.age).alias('age'), # get the sum of 'age'
        F.lit('All').alias('city') # create a column named 'city' and filled with 'All'
    ]))
res.show()

# Word count ==================================

# Count words split
poem.select(size(split(poem.value, "\s+")) \
	.alias("numWords")) \
	.agg(max(col("numWords"))) \
	.collect()
#> [Row(max(numWords)=288)]

# Perform word count, if sep by spaces:
# Creates list of Rows
wc = poem.select(explode(split(poem.value, "\s+")) \
	.alias("word")) \
	.groupBy("word") \
	.count() \
	.collect()
schema = StructType([StructField('word', StringType(), True),
                     StructField('count', IntegerType(), True)])

# Create DataFrame from list of Row types
wc2 = spark.createDataFrame(wc, schema=schema)

# Filter DataFrame for Row w 'if' in word column
wcIf = wc2.filter(wc2.word.isin('if'))
wcIf.show()

# Cache the DataFrame
wc2.cache()

# Perform word count, if sep by comma
wc = dog.select(explode(split(dog.value, ",")) \
	.alias("word"))
	.groupBy("word")
	.count()
	.collect()

# ==========================================================



